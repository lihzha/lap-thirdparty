(base) lihanzha@mae-lihanzha openpi-cot % tpu v4 "source ~/.zshrc && cd openpi-cot && git checkout tmp && git pull origin tmp && uv run --group tpu scripts/train.py pi_combined_cot_v4 --fsdp-devices=4 --batch-size=4 --exp-name=v4_combined_test --weight-loader.kind=checkpoint --weight-loader.params-path=gs://openpi-assets/checkpoints/pi05_base/params --data.use-wrist-image --model.pi05 --model.discrete_state_input --lr-schedule.peak-lr=0.0002 --lr-schedule.warmup-steps=1000 --lr-schedule.decay-lr=0.0002 --model.max_token_len=140 --data.no-vis-dataset --resume --data.shuffle-buffer-size=250000 --data.no-use-idle-filter"
SSH key found in project metadata; not updating instance.
Using ssh batch size of 1. Attempting to SSH into 1 nodes with a total of 1 workers.
SSH: Attempting to connect to worker 0...
Branch 'tmp' set up to track remote branch 'tmp' from 'origin'.
Switched to a new branch 'tmp'
From github-openpi-cot:lihzha/openpi-cot
 * branch            tmp        -> FETCH_HEAD
Already up to date.
warning: The `tool.uv.dev-dependencies` field (used in `third_party/openpi/packages/openpi-client/pyproject.toml`) is deprecated and will be removed in a future release; use `dependency-groups.dev` instead
Downloading libtpu (124.5MiB)
 Downloading libtpu
Installed 2 packages in 46ms
/home/lihanzha/openpi-cot/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
15:36:07.608 [I] Local devices: 4, Global devices: 4, Process count: 1                            (14970:train.py:344)
15:36:07.608 [I] Running on: t1v-n-6d1f5918-w-0                                                   (14970:train.py:364)
15:36:08.018 [I] Device mesh: mesh axes [batch=1, fsdp=4] total_devices=4                         (14970:mh_sharding.py:159)
15:36:08.018 [I] Data sharding spec: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(('batch', 'fsdp'),)) (14970:train.py:376)
15:36:08.019 [I] Replicated sharding spec: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) (14970:train.py:377)
15:36:08.019 [I] Checkpoint_dir:gs://pi0-cot/checkpoints/pi_combined_cot_v4/v4_combined_test      (14970:checkpoints.py:40)
15:36:08.019 [I] Checkpoint_dir:gs://pi0-cot/checkpoints/pi_combined_cot_v4/v4_combined_test      (14970:checkpoints.py:42)
15:36:08.118 [I] Created BasePyTreeCheckpointHandler: use_ocdbt=True, use_zarr3=False, pytree_metadata_options=PyTreeMetadataOptions(support_rich_types=False), array_metadata_store=<orbax.checkpoint._src.metadata.array_metadata_store.Store object at 0x7fb7874d6850> (14970:base_pytree_checkpoint_handler.py:334)
15:36:08.118 [I] Created BasePyTreeCheckpointHandler: use_ocdbt=True, use_zarr3=False, pytree_metadata_options=PyTreeMetadataOptions(support_rich_types=False), array_metadata_store=<orbax.checkpoint._src.metadata.array_metadata_store.Store object at 0x7fb7874d6850> (14970:base_pytree_checkpoint_handler.py:334)
15:36:08.118 [I] [thread=MainThread] Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID. (14970:multihost.py:390)
15:36:08.118 [I] [process=0][thread=MainThread] CheckpointManager init: checkpointers=None, item_names=None, item_handlers={'assets': <openpi_cot.training.checkpoints.CallbackHandler object at 0x7fb4d908cdd0>, 'train_state': <orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x7fb4d90142d0>, 'params': <orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x7fb5ca84bf10>}, handler_registry=None (14970:checkpoint_manager.py:620)
15:36:08.118 [I] Deferred registration for item: "assets". Adding handler `<openpi_cot.training.checkpoints.CallbackHandler object at 0x7fb4d908cdd0>` for item "assets" and save args `<class 'openpi_cot.training.checkpoints.CallbackSave'>` and restore args `<class 'openpi_cot.training.checkpoints.CallbackRestore'>` to `_handler_registry`. (14970:composite_checkpoint_handler.py:234)
15:36:08.118 [I] Deferred registration for item: "train_state". Adding handler `<orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x7fb4d90142d0>` for item "train_state" and save args `<class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeSaveArgs'>` and restore args `<class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeRestoreArgs'>` to `_handler_registry`. (14970:composite_checkpoint_handler.py:234)
15:36:08.118 [I] Deferred registration for item: "params". Adding handler `<orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x7fb5ca84bf10>` for item "params" and save args `<class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeSaveArgs'>` and restore args `<class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeRestoreArgs'>` to `_handler_registry`. (14970:composite_checkpoint_handler.py:234)
15:36:08.118 [I] Deferred registration for item: "metrics". Adding handler `<orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonCheckpointHandler object at 0x7fb4d9015e90>` for item "metrics" and save args `<class 'orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonSaveArgs'>` and restore args `<class 'orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonRestoreArgs'>` to `_handler_registry`. (14970:composite_checkpoint_handler.py:234)
15:36:08.118 [I] Initialized registry DefaultCheckpointHandlerRegistry({('assets', <class 'openpi_cot.training.checkpoints.CallbackSave'>): <openpi_cot.training.checkpoints.CallbackHandler object at 0x7fb4d908cdd0>, ('assets', <class 'openpi_cot.training.checkpoints.CallbackRestore'>): <openpi_cot.training.checkpoints.CallbackHandler object at 0x7fb4d908cdd0>, ('train_state', <class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeSaveArgs'>): <orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x7fb4d90142d0>, ('train_state', <class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeRestoreArgs'>): <orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x7fb4d90142d0>, ('params', <class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeSaveArgs'>): <orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x7fb5ca84bf10>, ('params', <class 'orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeRestoreArgs'>): <orbax.checkpoint._src.handlers.pytree_checkpoint_handler.PyTreeCheckpointHandler object at 0x7fb5ca84bf10>, ('metrics', <class 'orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonSaveArgs'>): <orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonCheckpointHandler object at 0x7fb4d9015e90>, ('metrics', <class 'orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonRestoreArgs'>): <orbax.checkpoint._src.handlers.json_checkpoint_handler.JsonCheckpointHandler object at 0x7fb4d9015e90>}). (14970:composite_checkpoint_handler.py:502)
15:36:08.119 [I] orbax-checkpoint version: 0.11.13                                                (14970:abstract_checkpointer.py:35)
15:36:08.119 [I] [process=0][thread=MainThread] Using barrier_sync_fn: <function get_barrier_sync_fn.<locals>.<lambda> at 0x7fb4d8d9c720> timeout: 7200 secs and primary_host=0 for async checkpoint writes (14970:async_checkpointer.py:170)
15:36:08.237 [I] Found 0 checkpoint steps in gs://pi0-cot/checkpoints/pi_combined_cot_v4/v4_combined_test (14970:checkpoint_manager.py:1701)
15:36:08.238 [I] [process=0][thread=MainThread] CheckpointManager created,  primary_host=0, CheckpointManagerOptions=CheckpointManagerOptions(save_interval_steps=1, max_to_keep=1, keep_time_interval=None, keep_period=5000, should_keep_fn=None, best_fn=None, best_mode='max', keep_checkpoints_without_metrics=True, step_prefix=None, step_format_fixed_length=None, step_name_format=None, create=False, cleanup_tmp_directories=False, save_on_steps=frozenset(), single_host_load_and_broadcast=False, todelete_subdir=None, enable_background_delete=False, read_only=False, enable_async_checkpointing=True, async_options=AsyncOptions(timeout_secs=7200, barrier_sync_fn=None, post_finalization_callback=None, create_directories_asynchronously=True), multiprocessing_options=MultiprocessingOptions(primary_host=0, active_processes=None, barrier_sync_key_prefix=None), should_save_fn=None, file_options=FileOptions(path_permission_mode=None), save_root_metadata=True, temporary_path_class=None, save_decision_policy=None, prevent_write_metrics=False), root_directory=gs://pi0-cot/checkpoints/pi_combined_cot_v4/v4_combined_test: <orbax.checkpoint.checkpoint_manager.CheckpointManager object at 0x7fb4d916bf50> (14970:checkpoint_manager.py:801)
15:36:08.238 [I] Checkpoint directory exists, but does not contain any checkpoints. Aborting resume. (14970:checkpoints.py:96)
wandb: Currently logged in as: lihanzha to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.22.0
wandb: Run data is saved locally in /home/lihanzha/openpi-cot/wandb/run-20250923_153608-5pgilzml
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run v4_combined_test
wandb: ‚≠êÔ∏è View project at https://wandb.ai/lihanzha/openpi-cot
wandb: üöÄ View run at https://wandb.ai/lihanzha/openpi-cot/runs/5pgilzml
15:36:09.315 [I] Before create data loader: RAM: 5.65GB, TPU mem: 0.00GB                          (14970:train.py:394)
15:36:09.316 [I] Downloading gs://big_vision/paligemma_tokenizer.model to /home/lihanzha/openpi-cot/gs:/pi0-cot/cache/big_vision/paligemma_tokenizer.model (14970:download.py:84)
15:36:10.101 [I] Progress on: -/4264023 rate:- remaining:? elapsed:00:00 postfix:-                (14970:tqdm_logging.py:145)
15:36:12.103 [I] Progress on: 4.07MiB/4.07MiB rate:2,486,791.8iB/s remaining:00:00 elapsed:00:02 postfix:- (14970:tqdm_logging.py:145)
15:36:12.219 [I] data_config: CoTDataConfig(repo_id='combined', asset_id='combined', norm_stats=None, repack_transforms=Group(inputs=(), outputs=()), data_transforms=Group(inputs=[CoTInputs(action_dim=32, sum_decimal='0f', wrist_image_dropout_prob=0.0, model_type=<ExtendedModelType.PI_COT: 'pi_cot'>)], outputs=[CoTOutputs()]), model_transforms=Group(inputs=[InjectDefaultPrompt(prompt=None), TokenizePromptAndReasoning(tokenizer=<openpi_cot.models.adapters.tokenizer_adapter.PaligemmaCoTTokenizer object at 0x7fb4d9201a10>, discrete_state_input=True), PadStatesAndActions(model_action_dim=32)], outputs=[DetokenizeReasoning(tokenizer=<openpi_cot.models.adapters.tokenizer_adapter.PaligemmaCoTTokenizer object at 0x7fb4d8f65150>)]), use_quantile_norm=False, action_sequence_keys=('actions',), prompt_from_task=False, rlds_data_dir='gs://pi0-cot/OXE', action_space=None, filter_dict_path=None, cot=False, shuffle_buffer_size=250000, summation_steps=15, max_samples=None, sum_decimal='0f', left_pad=True, include_decimal_point=True, val_max_samples=60000, val_fraction=0.02, validation_mode='easy', use_wrist_image=True, wrist_image_dropout_prob=0.0, dataset_type='combined', state_encoding=<StateEncoding.POS_EULER: 1>, action_encoding=<ActionEncoding.ABS_EEF_POS: 5>, resize_resolution=(224, 224), vis_dataset=False, use_idle_filter=False, use_per_traj_filter=False, drop_gripper_oob=False, language_action_dir='gs://pi0-cot/droid-base-lang-actions', droid_rlds_data_dir=None, droid_dataset_name='droid', data_mix='oxe_pi_magic_soup', droid_weight=2.0) (14970:cot_data_loader.py:121)
15:36:12.220 [I] Threads per Dataset: [-1 -1 -1]                                                  (14970:cot_rlds_dataset.py:1315)
15:36:12.221 [I] Reads per Dataset: [-1 -1 -1]                                                    (14970:cot_rlds_dataset.py:1316)
15:36:12.221 [I] Constructing datasets...                                                         (14970:cot_rlds_dataset.py:1319)
15:36:12.700 [I] Load dataset info from gs://gresearch/robotics/kuka/0.1.0                        (14970:dataset_info.py:707)
15:36:12.848 [I] Load dataset info from gs://pi0-cot/OXE/kuka/0.1.0                               (14970:dataset_info.py:707)
15:36:12.906 [I] For 'kuka/0.1.0': fields info.[release_notes, citation, location, module_name] differ on disk and in the code. Keeping the one from code. (14970:dataset_info.py:793)
15:36:12.906 [W] options.experimental_threading is deprecated. Use options.threading instead.     (14970:options.py:615)
15:36:12.980 [I] Creating a tf.data.Dataset reading 1024 files located in folders: gs://pi0-cot/OXE/kuka/0.1.0. (14970:reader.py:262)
15:36:13.070 [I] Constructing tf.data.Dataset kuka for split all, from gs://pi0-cot/OXE/kuka/0.1.0 (14970:logging_logger.py:49)
15:36:13.227 [I] Dataset kwargs: {'name': 'kuka', 'data_dir': 'gs://pi0-cot/OXE', 'image_obs_keys': {'primary': 'image', 'wrist': None}, 'state_obs_keys': ['clip_function_input/base_pose_tool_reached', 'gripper_closed'], 'state_encoding': <StateEncoding.POS_QUAT: 2>, 'action_encoding': <ActionEncoding.EEF_POS: 1>, 'action_proprio_normalization_type': <NormalizationType.NORMAL: 'normal'>, 'language_key': 'language_instruction', 'standardize_fn': <function kuka_dataset_transform at 0x7fb5ca7df920>} (14970:cot_rlds_dataset.py:993)
15:36:16.721 [I] Load dataset info from gs://gresearch/robotics/taco_play/0.1.0                   (14970:dataset_info.py:707)
15:36:16.876 [I] Load dataset info from gs://pi0-cot/OXE/taco_play/0.1.0                          (14970:dataset_info.py:707)
15:36:16.920 [W] options.experimental_threading is deprecated. Use options.threading instead.     (14970:options.py:615)
15:36:16.985 [I] Creating a tf.data.Dataset reading 575 files located in folders: gs://pi0-cot/OXE/taco_play/0.1.0. (14970:reader.py:262)
15:36:17.055 [I] Constructing tf.data.Dataset taco_play for split all, from gs://pi0-cot/OXE/taco_play/0.1.0 (14970:logging_logger.py:49)
15:36:17.126 [I] Dataset kwargs: {'name': 'taco_play', 'data_dir': 'gs://pi0-cot/OXE', 'image_obs_keys': {'primary': 'rgb_static', 'wrist': 'rgb_gripper'}, 'state_obs_keys': ['state_eef', 'state_gripper'], 'state_encoding': <StateEncoding.POS_EULER: 1>, 'action_encoding': <ActionEncoding.EEF_POS: 1>, 'action_proprio_normalization_type': <NormalizationType.NORMAL: 'normal'>, 'language_key': 'language_instruction', 'standardize_fn': <function taco_play_dataset_transform at 0x7fb5ca7df9c0>} (14970:cot_rlds_dataset.py:993)
15:36:17.897 [I] Load dataset info from gs://pi0-cot/OXE/furniture_bench_dataset_converted_externally_to_rlds/0.1.0 (14970:dataset_info.py:707)
15:36:18.124 [W] options.experimental_threading is deprecated. Use options.threading instead.     (14970:options.py:615)
15:36:18.175 [I] Creating a tf.data.Dataset reading 1024 files located in folders: gs://pi0-cot/OXE/furniture_bench_dataset_converted_externally_to_rlds/0.1.0. (14970:reader.py:262)
15:36:18.247 [I] Constructing tf.data.Dataset furniture_bench_dataset_converted_externally_to_rlds for split all, from gs://pi0-cot/OXE/furniture_bench_dataset_converted_externally_to_rlds/0.1.0 (14970:logging_logger.py:49)
15:36:18.372 [I] Dataset kwargs: {'name': 'furniture_bench_dataset_converted_externally_to_rlds', 'data_dir': 'gs://pi0-cot/OXE', 'image_obs_keys': {'primary': 'image', 'wrist': 'wrist_image'}, 'state_obs_keys': ['state'], 'state_encoding': <StateEncoding.POS_QUAT: 2>, 'action_encoding': <ActionEncoding.EEF_POS: 1>, 'action_proprio_normalization_type': <NormalizationType.NORMAL: 'normal'>, 'language_key': 'language_instruction', 'standardize_fn': <function furniture_bench_dataset_transform at 0x7fb5ca810400>} (14970:cot_rlds_dataset.py:993)
15:36:22.616 [I] Load dataset info from gs://pi0-cot/OXE/droid/1.0.1                              (14970:dataset_info.py:707)
15:36:22.710 [W] options.experimental_threading is deprecated. Use options.threading instead.     (14970:options.py:615)
15:36:22.773 [I] Creating a tf.data.Dataset reading 2048 files located in folders: gs://pi0-cot/OXE/droid/1.0.1. (14970:reader.py:262)
15:36:22.877 [I] Constructing tf.data.Dataset droid_101 for split all, from gs://pi0-cot/OXE/droid/1.0.1 (14970:logging_logger.py:49)
15:36:33.999 [I] [After building lang_table] Memory usage: 8720.07 MB                             (14970:cot_rlds_dataset.py:40)
15:36:34.213 [I] [After building ep_table] Memory usage: 8795.07 MB                               (14970:cot_rlds_dataset.py:40)
15:36:34.914 [I] [After building cam_table] Memory usage: 8844.89 MB                              (14970:cot_rlds_dataset.py:40)
15:36:39.137 [I] [After building instr_table] Memory usage: 8882.34 MB                            (14970:cot_rlds_dataset.py:40)
15:36:39.148 [I] Filter hash table initialized                                                    (14970:cot_rlds_dataset.py:533)
15:36:40.342 [I] Interleaving datasets...                                                         (14970:cot_rlds_dataset.py:1586)
15:36:40.364 [I] Applying frame transforms on dataset...                                          (14970:cot_rlds_dataset.py:1600)
15:36:40.696 [I] After create data loader: RAM: 8.64GB, TPU mem: 0.00GB                           (14970:train.py:406)
15:36:40.698 [I] After create data loader: RAM: 8.64GB, TPU mem: 0.00GB                           (14970:train.py:413)
15:36:40.698 [I] Before getting batch                                                             (14970:train.py:416)
15:37:54.803 [I] After getting batch: RAM: 251.52GB, TPU mem: 0.01GB                              (14970:train.py:432)
15:37:54.803 [I] After getting batch                                                              (14970:train.py:435)
15:37:54.804 [I] Initialized data loader (shapes):
[0].images['base_0_rgb']: (4, 224, 224, 3)@float32
[0].images['left_wrist_0_rgb']: (4, 224, 224, 3)@float32
[0].image_masks['base_0_rgb']: (4,)@bool
[0].image_masks['left_wrist_0_rgb']: (4,)@bool
[0].state: (4, 32)@float32
[0].tokenized_prompt: (4, 140)@int32
[0].tokenized_prompt_mask: (4, 140)@bool
[0].tokenized_reasoning_mask: (4, 140)@bool
[0].tokenized_numeric_mask: (4, 140)@bool
[0].example_mask: (4,)@bool
[1]: (4, 10, 32)@float32 (14970:train.py:436)
15:37:54.805 [I] Batch sharding summary:
[0].images['base_0_rgb']: global=(4, 224, 224, 3) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(('batch', 'fsdp'),)) | local_shard=(1, 224, 224, 3)
[0].images['left_wrist_0_rgb']: global=(4, 224, 224, 3) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(('batch', 'fsdp'),)) | local_shard=(1, 224, 224, 3)
[0].image_masks['base_0_rgb']: global=(4,) dtype=bool | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(('batch', 'fsdp'),)) | local_shard=(1,)
[0].image_masks['left_wrist_0_rgb']: global=(4,) dtype=bool | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(('batch', 'fsdp'),)) | local_shard=(1,)
[0].state: global=(4, 32) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(('batch', 'fsdp'),)) | local_shard=(1, 32)
[0].tokenized_prompt: global=(4, 140) dtype=int32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(('batch', 'fsdp'),)) | local_shard=(1, 140)
[0].tokenized_prompt_mask: global=(4, 140) dtype=bool | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(('batch', 'fsdp'),)) | local_shard=(1, 140)
[0].tokenized_reasoning_mask: global=(4, 140) dtype=bool | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(('batch', 'fsdp'),)) | local_shard=(1, 140)
[0].tokenized_numeric_mask: global=(4, 140) dtype=bool | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(('batch', 'fsdp'),)) | local_shard=(1, 140)
[0].example_mask: global=(4,) dtype=bool | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(('batch', 'fsdp'),)) | local_shard=(1,)
[1]: global=(4, 10, 32) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(('batch', 'fsdp'),)) | local_shard=(1, 10, 32) (14970:mh_sharding.py:181)
15:37:54.809 [I] Before init train state: RAM: 251.52GB, TPU mem: 0.01GB                          (14970:train.py:441)
15:37:56.594 [I] Sharding .params['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_0']['kernel'].value of shape (27, 1152, 4304) (510.68 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.595 [I] Sharding .params['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_1']['kernel'].value of shape (27, 4304, 1152) (510.68 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.595 [I] Sharding .params['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['key']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.595 [I] Sharding .params['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['out']['kernel'].value of shape (27, 16, 72, 1152) (136.69 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.595 [I] Sharding .params['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['query']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.596 [I] Sharding .params['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['value']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.596 [I] Sharding .params['PaliGemma']['img']['head']['kernel'].value of shape (1152, 2048) (9.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.596 [I] Sharding .params['PaliGemma']['llm']['embedder']['input_embedding'].value of shape (257152, 2048) (2009.00 MiB) along axis 0 (14970:sharding.py:89)
15:37:56.597 [I] Sharding .params['PaliGemma']['llm']['final_norm_1']['Dense_0']['kernel'].value of shape (1024, 3072) (12.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.597 [I] Sharding .params['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum']['w'].value of shape (18, 8, 256, 2048) (288.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.597 [I] Sharding .params['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum_1']['w'].value of shape (18, 8, 256, 1024) (144.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.597 [I] Sharding .params['PaliGemma']['llm']['layers']['attn']['kv_einsum']['w'].value of shape (18, 2, 1, 2048, 256) (72.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.597 [I] Sharding .params['PaliGemma']['llm']['layers']['attn']['kv_einsum_1']['w'].value of shape (18, 2, 1, 1024, 256) (36.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.598 [I] Sharding .params['PaliGemma']['llm']['layers']['attn']['q_einsum']['w'].value of shape (18, 8, 2048, 256) (288.00 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.598 [I] Sharding .params['PaliGemma']['llm']['layers']['attn']['q_einsum_1']['w'].value of shape (18, 8, 1024, 256) (144.00 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.598 [I] Sharding .params['PaliGemma']['llm']['layers']['mlp']['gating_einsum'].value of shape (18, 2, 2048, 16384) (4608.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.599 [I] Sharding .params['PaliGemma']['llm']['layers']['mlp']['linear'].value of shape (18, 16384, 2048) (2304.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.599 [I] Sharding .params['PaliGemma']['llm']['layers']['mlp_1']['gating_einsum'].value of shape (18, 2, 1024, 4096) (576.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.599 [I] Sharding .params['PaliGemma']['llm']['layers']['mlp_1']['linear'].value of shape (18, 4096, 1024) (288.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.599 [I] Sharding .params['PaliGemma']['llm']['layers']['pre_attention_norm_1']['Dense_0']['kernel'].value of shape (18, 1024, 3072) (216.00 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.600 [I] Sharding .params['PaliGemma']['llm']['layers']['pre_ffw_norm_1']['Dense_0']['kernel'].value of shape (18, 1024, 3072) (216.00 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.600 [I] Sharding .params['time_mlp_in']['kernel'].value of shape (1024, 1024) (4.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.600 [I] Sharding .params['time_mlp_out']['kernel'].value of shape (1024, 1024) (4.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.600 [I] Sharding .opt_state[1][0].mu['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_0']['kernel'].value of shape (27, 1152, 4304) (510.68 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.601 [I] Sharding .opt_state[1][0].mu['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_1']['kernel'].value of shape (27, 4304, 1152) (510.68 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.601 [I] Sharding .opt_state[1][0].mu['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['key']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.601 [I] Sharding .opt_state[1][0].mu['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['out']['kernel'].value of shape (27, 16, 72, 1152) (136.69 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.601 [I] Sharding .opt_state[1][0].mu['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['query']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.602 [I] Sharding .opt_state[1][0].mu['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['value']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.602 [I] Sharding .opt_state[1][0].mu['PaliGemma']['img']['head']['kernel'].value of shape (1152, 2048) (9.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.602 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['embedder']['input_embedding'].value of shape (257152, 2048) (2009.00 MiB) along axis 0 (14970:sharding.py:89)
15:37:56.603 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['final_norm_1']['Dense_0']['kernel'].value of shape (1024, 3072) (12.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.603 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum']['w'].value of shape (18, 8, 256, 2048) (288.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.603 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum_1']['w'].value of shape (18, 8, 256, 1024) (144.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.603 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['attn']['kv_einsum']['w'].value of shape (18, 2, 1, 2048, 256) (72.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.603 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['attn']['kv_einsum_1']['w'].value of shape (18, 2, 1, 1024, 256) (36.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.604 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['attn']['q_einsum']['w'].value of shape (18, 8, 2048, 256) (288.00 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.604 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['attn']['q_einsum_1']['w'].value of shape (18, 8, 1024, 256) (144.00 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.604 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['mlp']['gating_einsum'].value of shape (18, 2, 2048, 16384) (4608.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.604 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['mlp']['linear'].value of shape (18, 16384, 2048) (2304.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.605 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['mlp_1']['gating_einsum'].value of shape (18, 2, 1024, 4096) (576.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.605 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['mlp_1']['linear'].value of shape (18, 4096, 1024) (288.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.605 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['pre_attention_norm_1']['Dense_0']['kernel'].value of shape (18, 1024, 3072) (216.00 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.605 [I] Sharding .opt_state[1][0].mu['PaliGemma']['llm']['layers']['pre_ffw_norm_1']['Dense_0']['kernel'].value of shape (18, 1024, 3072) (216.00 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.606 [I] Sharding .opt_state[1][0].mu['time_mlp_in']['kernel'].value of shape (1024, 1024) (4.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.606 [I] Sharding .opt_state[1][0].mu['time_mlp_out']['kernel'].value of shape (1024, 1024) (4.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.606 [I] Sharding .opt_state[1][0].nu['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_0']['kernel'].value of shape (27, 1152, 4304) (510.68 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.606 [I] Sharding .opt_state[1][0].nu['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_1']['kernel'].value of shape (27, 4304, 1152) (510.68 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.607 [I] Sharding .opt_state[1][0].nu['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['key']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.607 [I] Sharding .opt_state[1][0].nu['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['out']['kernel'].value of shape (27, 16, 72, 1152) (136.69 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.607 [I] Sharding .opt_state[1][0].nu['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['query']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.607 [I] Sharding .opt_state[1][0].nu['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['value']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.608 [I] Sharding .opt_state[1][0].nu['PaliGemma']['img']['head']['kernel'].value of shape (1152, 2048) (9.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.608 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['embedder']['input_embedding'].value of shape (257152, 2048) (2009.00 MiB) along axis 0 (14970:sharding.py:89)
15:37:56.608 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['final_norm_1']['Dense_0']['kernel'].value of shape (1024, 3072) (12.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.608 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum']['w'].value of shape (18, 8, 256, 2048) (288.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.609 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum_1']['w'].value of shape (18, 8, 256, 1024) (144.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.609 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['attn']['kv_einsum']['w'].value of shape (18, 2, 1, 2048, 256) (72.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.609 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['attn']['kv_einsum_1']['w'].value of shape (18, 2, 1, 1024, 256) (36.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.609 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['attn']['q_einsum']['w'].value of shape (18, 8, 2048, 256) (288.00 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.609 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['attn']['q_einsum_1']['w'].value of shape (18, 8, 1024, 256) (144.00 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.610 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['mlp']['gating_einsum'].value of shape (18, 2, 2048, 16384) (4608.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.610 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['mlp']['linear'].value of shape (18, 16384, 2048) (2304.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.610 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['mlp_1']['gating_einsum'].value of shape (18, 2, 1024, 4096) (576.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.611 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['mlp_1']['linear'].value of shape (18, 4096, 1024) (288.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.611 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['pre_attention_norm_1']['Dense_0']['kernel'].value of shape (18, 1024, 3072) (216.00 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.611 [I] Sharding .opt_state[1][0].nu['PaliGemma']['llm']['layers']['pre_ffw_norm_1']['Dense_0']['kernel'].value of shape (18, 1024, 3072) (216.00 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.611 [I] Sharding .opt_state[1][0].nu['time_mlp_in']['kernel'].value of shape (1024, 1024) (4.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.612 [I] Sharding .opt_state[1][0].nu['time_mlp_out']['kernel'].value of shape (1024, 1024) (4.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.612 [I] Sharding .ema_params['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_0']['kernel'].value of shape (27, 1152, 4304) (510.68 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.612 [I] Sharding .ema_params['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_1']['kernel'].value of shape (27, 4304, 1152) (510.68 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.612 [I] Sharding .ema_params['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['key']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.613 [I] Sharding .ema_params['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['out']['kernel'].value of shape (27, 16, 72, 1152) (136.69 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.613 [I] Sharding .ema_params['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['query']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.613 [I] Sharding .ema_params['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['value']['kernel'].value of shape (27, 1152, 16, 72) (136.69 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.613 [I] Sharding .ema_params['PaliGemma']['img']['head']['kernel'].value of shape (1152, 2048) (9.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.613 [I] Sharding .ema_params['PaliGemma']['llm']['embedder']['input_embedding'].value of shape (257152, 2048) (2009.00 MiB) along axis 0 (14970:sharding.py:89)
15:37:56.613 [I] Sharding .ema_params['PaliGemma']['llm']['final_norm_1']['Dense_0']['kernel'].value of shape (1024, 3072) (12.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.614 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum']['w'].value of shape (18, 8, 256, 2048) (288.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.614 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum_1']['w'].value of shape (18, 8, 256, 1024) (144.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.614 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['attn']['kv_einsum']['w'].value of shape (18, 2, 1, 2048, 256) (72.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.614 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['attn']['kv_einsum_1']['w'].value of shape (18, 2, 1, 1024, 256) (36.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.614 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['attn']['q_einsum']['w'].value of shape (18, 8, 2048, 256) (288.00 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.615 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['attn']['q_einsum_1']['w'].value of shape (18, 8, 1024, 256) (144.00 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.615 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['mlp']['gating_einsum'].value of shape (18, 2, 2048, 16384) (4608.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.615 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['mlp']['linear'].value of shape (18, 16384, 2048) (2304.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.616 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['mlp_1']['gating_einsum'].value of shape (18, 2, 1024, 4096) (576.00 MiB) along axis 3 (14970:sharding.py:89)
15:37:56.616 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['mlp_1']['linear'].value of shape (18, 4096, 1024) (288.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.616 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['pre_attention_norm_1']['Dense_0']['kernel'].value of shape (18, 1024, 3072) (216.00 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.616 [I] Sharding .ema_params['PaliGemma']['llm']['layers']['pre_ffw_norm_1']['Dense_0']['kernel'].value of shape (18, 1024, 3072) (216.00 MiB) along axis 2 (14970:sharding.py:89)
15:37:56.616 [I] Sharding .ema_params['time_mlp_in']['kernel'].value of shape (1024, 1024) (4.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.617 [I] Sharding .ema_params['time_mlp_out']['kernel'].value of shape (1024, 1024) (4.00 MiB) along axis 1 (14970:sharding.py:89)
15:37:56.802 [I] Created BasePyTreeCheckpointHandler: use_ocdbt=True, use_zarr3=False, pytree_metadata_options=PyTreeMetadataOptions(support_rich_types=False), array_metadata_store=<orbax.checkpoint._src.metadata.array_metadata_store.Store object at 0x7fb7874d6850> (14970:base_pytree_checkpoint_handler.py:334)
I0923 15:37:57.091877   19940 google_auth_provider.cc:181] Running on GCE, using service account 404383966826-compute@developer.gserviceaccount.com
15:37:57.482 [I] Restoring checkpoint from gs://pi0-cot/cache/openpi-assets/checkpoints/pi05_base/params. (14970:checkpointer.py:298)
15:38:17.440 [I] [process=0] /jax/checkpoint/read/bytes_per_sec: 646.9 MiB/s (total bytes: 12.5 GiB) (time elapsed: 19 seconds) (per-host) (14970:base_pytree_checkpoint_handler.py:114)
15:38:17.441 [I] Finished restoring checkpoint in 20.07 seconds from gs://pi0-cot/cache/openpi-assets/checkpoints/pi05_base/params. (14970:checkpointer.py:309)
/home/lihanzha/openpi-cot/.venv/lib/python3.11/site-packages/jax/_src/interpreters/mlir.py:1178: UserWarning: Some donated buffers were not usable: ShapedArray(float32[27,1152,4304]), ShapedArray(float32[27,4304,1152]), ShapedArray(float32[27,1152,16,72]), ShapedArray(float32[27,16,72,1152]), ShapedArray(float32[27,1152,16,72]), ShapedArray(float32[27,1152,16,72]), ShapedArray(float32[1152,2048]), ShapedArray(float32[257152,2048]), ShapedArray(float32[1024,3072]), ShapedArray(float32[18,8,256,2048]), ShapedArray(float32[18,8,256,1024]), ShapedArray(float32[18,2,1,2048,256]), ShapedArray(float32[18,2,1,1024,256]), ShapedArray(float32[18,8,2048,256]), ShapedArray(float32[18,8,1024,256]), ShapedArray(float32[18,2,2048,16384]), ShapedArray(float32[18,16384,2048]), ShapedArray(float32[18,2,1024,4096]), ShapedArray(float32[18,4096,1024]), ShapedArray(float32[18,1024,3072]), ShapedArray(float32[18,1024,3072]), ShapedArray(float32[1024,1024]), ShapedArray(float32[1024,1024]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn("Some donated buffers were not usable:"
15:38:22.398 [I] After init train state: RAM: 324.18GB, TPU mem: 50.80GB                          (14970:train.py:448)
15:38:22.400 [I] Initialized train state (param shapes):
['PaliGemma']['img']['Transformer']['encoder_norm']['bias'].value: (1152,)@float32
['PaliGemma']['img']['Transformer']['encoder_norm']['scale'].value: (1152,)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_0']['bias'].value: (27, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_0']['scale'].value: (27, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_1']['bias'].value: (27, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_1']['scale'].value: (27, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_0']['bias'].value: (27, 4304)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_0']['kernel'].value: (27, 1152, 4304)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_1']['bias'].value: (27, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_1']['kernel'].value: (27, 4304, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['key']['bias'].value: (27, 16, 72)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['key']['kernel'].value: (27, 1152, 16, 72)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['out']['bias'].value: (27, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['out']['kernel'].value: (27, 16, 72, 1152)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['query']['bias'].value: (27, 16, 72)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['query']['kernel'].value: (27, 1152, 16, 72)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['value']['bias'].value: (27, 16, 72)@float32
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['value']['kernel'].value: (27, 1152, 16, 72)@float32
['PaliGemma']['img']['embedding']['bias'].value: (1152,)@float32
['PaliGemma']['img']['embedding']['kernel'].value: (14, 14, 3, 1152)@float32
['PaliGemma']['img']['head']['bias'].value: (2048,)@float32
['PaliGemma']['img']['head']['kernel'].value: (1152, 2048)@float32
['PaliGemma']['img']['pos_embedding'].value: (1, 256, 1152)@float32
['PaliGemma']['llm']['embedder']['input_embedding'].value: (257152, 2048)@float32
['PaliGemma']['llm']['final_norm']['scale'].value: (2048,)@float32
['PaliGemma']['llm']['final_norm_1']['Dense_0']['bias'].value: (3072,)@float32
['PaliGemma']['llm']['final_norm_1']['Dense_0']['kernel'].value: (1024, 3072)@float32
['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum']['w'].value: (18, 8, 256, 2048)@float32
['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum_1']['w'].value: (18, 8, 256, 1024)@float32
['PaliGemma']['llm']['layers']['attn']['kv_einsum']['w'].value: (18, 2, 1, 2048, 256)@float32
['PaliGemma']['llm']['layers']['attn']['kv_einsum_1']['w'].value: (18, 2, 1, 1024, 256)@float32
['PaliGemma']['llm']['layers']['attn']['q_einsum']['w'].value: (18, 8, 2048, 256)@float32
['PaliGemma']['llm']['layers']['attn']['q_einsum_1']['w'].value: (18, 8, 1024, 256)@float32
['PaliGemma']['llm']['layers']['mlp']['gating_einsum'].value: (18, 2, 2048, 16384)@float32
['PaliGemma']['llm']['layers']['mlp']['linear'].value: (18, 16384, 2048)@float32
['PaliGemma']['llm']['layers']['mlp_1']['gating_einsum'].value: (18, 2, 1024, 4096)@float32
['PaliGemma']['llm']['layers']['mlp_1']['linear'].value: (18, 4096, 1024)@float32
['PaliGemma']['llm']['layers']['pre_attention_norm']['scale'].value: (18, 2048)@float32
['PaliGemma']['llm']['layers']['pre_attention_norm_1']['Dense_0']['bias'].value: (18, 3072)@float32
['PaliGemma']['llm']['layers']['pre_attention_norm_1']['Dense_0']['kernel'].value: (18, 1024, 3072)@float32
['PaliGemma']['llm']['layers']['pre_ffw_norm']['scale'].value: (18, 2048)@float32
['PaliGemma']['llm']['layers']['pre_ffw_norm_1']['Dense_0']['bias'].value: (18, 3072)@float32
['PaliGemma']['llm']['layers']['pre_ffw_norm_1']['Dense_0']['kernel'].value: (18, 1024, 3072)@float32
['action_in_proj']['bias'].value: (1024,)@float32
['action_in_proj']['kernel'].value: (32, 1024)@float32
['action_out_proj']['bias'].value: (32,)@float32
['action_out_proj']['kernel'].value: (1024, 32)@float32
['time_mlp_in']['bias'].value: (1024,)@float32
['time_mlp_in']['kernel'].value: (1024, 1024)@float32
['time_mlp_out']['bias'].value: (1024,)@float32
['time_mlp_out']['kernel'].value: (1024, 1024)@float32 (14970:train.py:450)
15:38:22.401 [I] Planned parameter sharding (from fsdp_sharding): sharded=23 replicated=28
['PaliGemma']['img']['Transformer']['encoder_norm']['bias'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['img']['Transformer']['encoder_norm']['scale'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_0']['bias'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_0']['scale'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_1']['bias'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_1']['scale'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_0']['bias'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_0']['kernel'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, 'fsdp'))
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_1']['bias'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_1']['kernel'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp', None))
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['key']['bias'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['key']['kernel'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp', None, None))
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['out']['bias'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['out']['kernel'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, None, 'fsdp'))
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['query']['bias'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['query']['kernel'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp', None, None))
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['value']['bias'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['value']['kernel'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp', None, None))
['PaliGemma']['img']['embedding']['bias'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['img']['embedding']['kernel'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['img']['head']['bias'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['img']['head']['kernel'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp'))
['PaliGemma']['img']['pos_embedding'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['llm']['embedder']['input_embedding'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec('fsdp', None))
['PaliGemma']['llm']['final_norm']['scale'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['llm']['final_norm_1']['Dense_0']['bias'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['llm']['final_norm_1']['Dense_0']['kernel'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp'))
['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum']['w'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, None, 'fsdp'))
['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum_1']['w'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, None, 'fsdp'))
['PaliGemma']['llm']['layers']['attn']['kv_einsum']['w'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, None, 'fsdp', None))
['PaliGemma']['llm']['layers']['attn']['kv_einsum_1']['w'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, None, 'fsdp', None))
['PaliGemma']['llm']['layers']['attn']['q_einsum']['w'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, 'fsdp', None))
['PaliGemma']['llm']['layers']['attn']['q_einsum_1']['w'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, 'fsdp', None))
['PaliGemma']['llm']['layers']['mlp']['gating_einsum'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, None, 'fsdp'))
['PaliGemma']['llm']['layers']['mlp']['linear'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp', None))
['PaliGemma']['llm']['layers']['mlp_1']['gating_einsum'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, None, 'fsdp'))
['PaliGemma']['llm']['layers']['mlp_1']['linear'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp', None))
['PaliGemma']['llm']['layers']['pre_attention_norm']['scale'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['llm']['layers']['pre_attention_norm_1']['Dense_0']['bias'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['llm']['layers']['pre_attention_norm_1']['Dense_0']['kernel'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, 'fsdp'))
['PaliGemma']['llm']['layers']['pre_ffw_norm']['scale'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['llm']['layers']['pre_ffw_norm_1']['Dense_0']['bias'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['PaliGemma']['llm']['layers']['pre_ffw_norm_1']['Dense_0']['kernel'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, 'fsdp'))
['action_in_proj']['bias'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['action_in_proj']['kernel'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['action_out_proj']['bias'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['action_out_proj']['kernel'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['time_mlp_in']['bias'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['time_mlp_in']['kernel'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp'))
['time_mlp_out']['bias'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec())
['time_mlp_out']['kernel'].value: NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp')) (14970:mh_sharding.py:206)
15:38:22.404 [I] Actual parameter sharding:
['PaliGemma']['img']['Transformer']['encoder_norm']['bias'].value: global=(1152,) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(1152,)
['PaliGemma']['img']['Transformer']['encoder_norm']['scale'].value: global=(1152,) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(1152,)
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_0']['bias'].value: global=(27, 1152) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(27, 1152)
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_0']['scale'].value: global=(27, 1152) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(27, 1152)
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_1']['bias'].value: global=(27, 1152) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(27, 1152)
['PaliGemma']['img']['Transformer']['encoderblock']['LayerNorm_1']['scale'].value: global=(27, 1152) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(27, 1152)
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_0']['bias'].value: global=(27, 4304) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(27, 4304)
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_0']['kernel'].value: global=(27, 1152, 4304) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, 'fsdp')) | local_shard=(27, 1152, 1076)
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_1']['bias'].value: global=(27, 1152) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(27, 1152)
['PaliGemma']['img']['Transformer']['encoderblock']['MlpBlock_0']['Dense_1']['kernel'].value: global=(27, 4304, 1152) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp', None)) | local_shard=(27, 1076, 1152)
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['key']['bias'].value: global=(27, 16, 72) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(27, 16, 72)
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['key']['kernel'].value: global=(27, 1152, 16, 72) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp', None, None)) | local_shard=(27, 288, 16, 72)
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['out']['bias'].value: global=(27, 1152) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(27, 1152)
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['out']['kernel'].value: global=(27, 16, 72, 1152) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, None, 'fsdp')) | local_shard=(27, 16, 72, 288)
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['query']['bias'].value: global=(27, 16, 72) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(27, 16, 72)
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['query']['kernel'].value: global=(27, 1152, 16, 72) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp', None, None)) | local_shard=(27, 288, 16, 72)
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['value']['bias'].value: global=(27, 16, 72) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(27, 16, 72)
['PaliGemma']['img']['Transformer']['encoderblock']['MultiHeadDotProductAttention_0']['value']['kernel'].value: global=(27, 1152, 16, 72) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp', None, None)) | local_shard=(27, 288, 16, 72)
['PaliGemma']['img']['embedding']['bias'].value: global=(1152,) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(1152,)
['PaliGemma']['img']['embedding']['kernel'].value: global=(14, 14, 3, 1152) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(14, 14, 3, 1152)
['PaliGemma']['img']['head']['bias'].value: global=(2048,) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(2048,)
['PaliGemma']['img']['head']['kernel'].value: global=(1152, 2048) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp')) | local_shard=(1152, 512)
['PaliGemma']['img']['pos_embedding'].value: global=(1, 256, 1152) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(1, 256, 1152)
['PaliGemma']['llm']['embedder']['input_embedding'].value: global=(257152, 2048) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec('fsdp', None)) | local_shard=(64288, 2048)
['PaliGemma']['llm']['final_norm']['scale'].value: global=(2048,) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(2048,)
['PaliGemma']['llm']['final_norm_1']['Dense_0']['bias'].value: global=(3072,) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(3072,)
['PaliGemma']['llm']['final_norm_1']['Dense_0']['kernel'].value: global=(1024, 3072) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp')) | local_shard=(1024, 768)
['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum']['w'].value: global=(18, 8, 256, 2048) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, None, 'fsdp')) | local_shard=(18, 8, 256, 512)
['PaliGemma']['llm']['layers']['attn']['attn_vec_einsum_1']['w'].value: global=(18, 8, 256, 1024) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, None, 'fsdp')) | local_shard=(18, 8, 256, 256)
['PaliGemma']['llm']['layers']['attn']['kv_einsum']['w'].value: global=(18, 2, 1, 2048, 256) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, None, 'fsdp', None)) | local_shard=(18, 2, 1, 512, 256)
['PaliGemma']['llm']['layers']['attn']['kv_einsum_1']['w'].value: global=(18, 2, 1, 1024, 256) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, None, 'fsdp', None)) | local_shard=(18, 2, 1, 256, 256)
['PaliGemma']['llm']['layers']['attn']['q_einsum']['w'].value: global=(18, 8, 2048, 256) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, 'fsdp', None)) | local_shard=(18, 8, 512, 256)
['PaliGemma']['llm']['layers']['attn']['q_einsum_1']['w'].value: global=(18, 8, 1024, 256) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, 'fsdp', None)) | local_shard=(18, 8, 256, 256)
['PaliGemma']['llm']['layers']['mlp']['gating_einsum'].value: global=(18, 2, 2048, 16384) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, None, 'fsdp')) | local_shard=(18, 2, 2048, 4096)
['PaliGemma']['llm']['layers']['mlp']['linear'].value: global=(18, 16384, 2048) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp', None)) | local_shard=(18, 4096, 2048)
['PaliGemma']['llm']['layers']['mlp_1']['gating_einsum'].value: global=(18, 2, 1024, 4096) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, None, 'fsdp')) | local_shard=(18, 2, 1024, 1024)
['PaliGemma']['llm']['layers']['mlp_1']['linear'].value: global=(18, 4096, 1024) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp', None)) | local_shard=(18, 1024, 1024)
['PaliGemma']['llm']['layers']['pre_attention_norm']['scale'].value: global=(18, 2048) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(18, 2048)
['PaliGemma']['llm']['layers']['pre_attention_norm_1']['Dense_0']['bias'].value: global=(18, 3072) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(18, 3072)
['PaliGemma']['llm']['layers']['pre_attention_norm_1']['Dense_0']['kernel'].value: global=(18, 1024, 3072) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, 'fsdp')) | local_shard=(18, 1024, 768)
['PaliGemma']['llm']['layers']['pre_ffw_norm']['scale'].value: global=(18, 2048) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(18, 2048)
['PaliGemma']['llm']['layers']['pre_ffw_norm_1']['Dense_0']['bias'].value: global=(18, 3072) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(18, 3072)
['PaliGemma']['llm']['layers']['pre_ffw_norm_1']['Dense_0']['kernel'].value: global=(18, 1024, 3072) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, None, 'fsdp')) | local_shard=(18, 1024, 768)
['action_in_proj']['bias'].value: global=(1024,) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(1024,)
['action_in_proj']['kernel'].value: global=(32, 1024) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(32, 1024)
['action_out_proj']['bias'].value: global=(32,) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(32,)
['action_out_proj']['kernel'].value: global=(1024, 32) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(1024, 32)
['time_mlp_in']['bias'].value: global=(1024,) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(1024,)
['time_mlp_in']['kernel'].value: global=(1024, 1024) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp')) | local_shard=(1024, 256)
['time_mlp_out']['bias'].value: global=(1024,) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec()) | local_shard=(1024,)
['time_mlp_out']['kernel'].value: global=(1024, 1024) dtype=float32 | NamedSharding(mesh=[batch=1, fsdp=4], spec=PartitionSpec(None, 'fsdp')) | local_shard=(1024, 256) (14970:mh_sharding.py:229)
15:38:22.420 [I] After ptrain_step: RAM: 324.18GB, TPU mem: 50.80GB                               (14970:train.py:467)
15:38:22.423 [I] Before create val loader: RAM: 324.18GB, TPU mem: 50.80GB                        (14970:train.py:473)
15:38:22.724 [I] data_config: CoTDataConfig(repo_id='combined', asset_id='combined', norm_stats=None, repack_transforms=Group(inputs=(), outputs=()), data_transforms=Group(inputs=[CoTInputs(action_dim=32, sum_decimal='0f', wrist_image_dropout_prob=0.0, model_type=<ExtendedModelType.PI_COT: 'pi_cot'>)], outputs=[CoTOutputs()]), model_transforms=Group(inputs=[InjectDefaultPrompt(prompt=None), TokenizePromptAndReasoning(tokenizer=<openpi_cot.models.adapters.tokenizer_adapter.PaligemmaCoTTokenizer object at 0x7fa35997a1d0>, discrete_state_input=True), PadStatesAndActions(model_action_dim=32)], outputs=[DetokenizeReasoning(tokenizer=<openpi_cot.models.adapters.tokenizer_adapter.PaligemmaCoTTokenizer object at 0x7fb27cbcc590>)]), use_quantile_norm=False, action_sequence_keys=('actions',), prompt_from_task=False, rlds_data_dir='gs://pi0-cot/OXE', action_space=None, filter_dict_path=None, cot=False, shuffle_buffer_size=250000, summation_steps=15, max_samples=None, sum_decimal='0f', left_pad=True, include_decimal_point=True, val_max_samples=60000, val_fraction=0.02, validation_mode='easy', use_wrist_image=True, wrist_image_dropout_prob=0.0, dataset_type='combined', state_encoding=<StateEncoding.POS_EULER: 1>, action_encoding=<ActionEncoding.ABS_EEF_POS: 5>, resize_resolution=(224, 224), vis_dataset=False, use_idle_filter=False, use_per_traj_filter=False, drop_gripper_oob=False, language_action_dir='gs://pi0-cot/droid-base-lang-actions', droid_rlds_data_dir=None, droid_dataset_name='droid', data_mix='oxe_pi_magic_soup', droid_weight=2.0) (14970:cot_data_loader.py:121)
15:38:22.725 [I] Threads per Dataset: [-1 -1 -1]                                                  (14970:cot_rlds_dataset.py:1315)
15:38:22.725 [I] Reads per Dataset: [-1 -1 -1]                                                    (14970:cot_rlds_dataset.py:1316)
15:38:22.725 [I] Constructing datasets...                                                         (14970:cot_rlds_dataset.py:1319)
15:38:22.849 [I] Load dataset info from gs://gresearch/robotics/kuka/0.1.0                        (14970:dataset_info.py:707)
15:38:23.000 [I] Load dataset info from gs://pi0-cot/OXE/kuka/0.1.0                               (14970:dataset_info.py:707)
15:38:23.049 [I] For 'kuka/0.1.0': fields info.[release_notes, citation, location, module_name] differ on disk and in the code. Keeping the one from code. (14970:dataset_info.py:793)
15:38:23.049 [W] options.experimental_threading is deprecated. Use options.threading instead.     (14970:options.py:615)
15:38:23.122 [I] Creating a tf.data.Dataset reading 1024 files located in folders: gs://pi0-cot/OXE/kuka/0.1.0. (14970:reader.py:262)
15:38:23.283 [I] Constructing tf.data.Dataset kuka for split all, from gs://pi0-cot/OXE/kuka/0.1.0 (14970:logging_logger.py:49)
2025-09-23 15:38:23.387433: W tensorflow/core/framework/dataset.cc:736] Changing the value of option field deterministic from 0 to 1
15:38:23.387 [W] Changing the value of option deterministic from False to True.                   (14970:options.py:170)
15:38:23.388 [I] Dataset kwargs: {'name': 'kuka', 'data_dir': 'gs://pi0-cot/OXE', 'image_obs_keys': {'primary': 'image', 'wrist': None}, 'state_obs_keys': ['clip_function_input/base_pose_tool_reached', 'gripper_closed'], 'state_encoding': <StateEncoding.POS_QUAT: 2>, 'action_encoding': <ActionEncoding.EEF_POS: 1>, 'action_proprio_normalization_type': <NormalizationType.NORMAL: 'normal'>, 'language_key': 'language_instruction', 'standardize_fn': <function kuka_dataset_transform at 0x7fb5ca7df920>} (14970:cot_rlds_dataset.py:993)
15:38:24.103 [I] Load dataset info from gs://gresearch/robotics/taco_play/0.1.0                   (14970:dataset_info.py:707)
15:38:24.253 [I] Load dataset info from gs://pi0-cot/OXE/taco_play/0.1.0                          (14970:dataset_info.py:707)
15:38:24.356 [W] options.experimental_threading is deprecated. Use options.threading instead.     (14970:options.py:615)
15:38:24.421 [I] Creating a tf.data.Dataset reading 575 files located in folders: gs://pi0-cot/OXE/taco_play/0.1.0. (14970:reader.py:262)
15:38:24.501 [I] Constructing tf.data.Dataset taco_play for split all, from gs://pi0-cot/OXE/taco_play/0.1.0 (14970:logging_logger.py:49)
2025-09-23 15:38:24.576772: W tensorflow/core/framework/dataset.cc:736] Changing the value of option field deterministic from 0 to 1
15:38:24.577 [W] Changing the value of option deterministic from False to True.                   (14970:options.py:170)
15:38:24.577 [I] Dataset kwargs: {'name': 'taco_play', 'data_dir': 'gs://pi0-cot/OXE', 'image_obs_keys': {'primary': 'rgb_static', 'wrist': 'rgb_gripper'}, 'state_obs_keys': ['state_eef', 'state_gripper'], 'state_encoding': <StateEncoding.POS_EULER: 1>, 'action_encoding': <ActionEncoding.EEF_POS: 1>, 'action_proprio_normalization_type': <NormalizationType.NORMAL: 'normal'>, 'language_key': 'language_instruction', 'standardize_fn': <function taco_play_dataset_transform at 0x7fb5ca7df9c0>} (14970:cot_rlds_dataset.py:993)
15:38:25.228 [I] Load dataset info from gs://pi0-cot/OXE/furniture_bench_dataset_converted_externally_to_rlds/0.1.0 (14970:dataset_info.py:707)
15:38:25.425 [W] options.experimental_threading is deprecated. Use options.threading instead.     (14970:options.py:615)
15:38:25.473 [I] Creating a tf.data.Dataset reading 1024 files located in folders: gs://pi0-cot/OXE/furniture_bench_dataset_converted_externally_to_rlds/0.1.0. (14970:reader.py:262)
15:38:25.555 [I] Constructing tf.data.Dataset furniture_bench_dataset_converted_externally_to_rlds for split all, from gs://pi0-cot/OXE/furniture_bench_dataset_converted_externally_to_rlds/0.1.0 (14970:logging_logger.py:49)
2025-09-23 15:38:25.690349: W tensorflow/core/framework/dataset.cc:736] Changing the value of option field deterministic from 0 to 1
15:38:25.690 [W] Changing the value of option deterministic from False to True.                   (14970:options.py:170)
15:38:25.691 [I] Dataset kwargs: {'name': 'furniture_bench_dataset_converted_externally_to_rlds', 'data_dir': 'gs://pi0-cot/OXE', 'image_obs_keys': {'primary': 'image', 'wrist': 'wrist_image'}, 'state_obs_keys': ['state'], 'state_encoding': <StateEncoding.POS_QUAT: 2>, 'action_encoding': <ActionEncoding.EEF_POS: 1>, 'action_proprio_normalization_type': <NormalizationType.NORMAL: 'normal'>, 'language_key': 'language_instruction', 'standardize_fn': <function furniture_bench_dataset_transform at 0x7fb5ca810400>} (14970:cot_rlds_dataset.py:993)
15:38:26.486 [I] Load dataset info from gs://pi0-cot/OXE/droid/1.0.1                              (14970:dataset_info.py:707)
15:38:26.570 [W] options.experimental_threading is deprecated. Use options.threading instead.     (14970:options.py:615)
15:38:26.630 [I] Creating a tf.data.Dataset reading 2048 files located in folders: gs://pi0-cot/OXE/droid/1.0.1. (14970:reader.py:262)
15:38:26.742 [I] Constructing tf.data.Dataset droid_101 for split all, from gs://pi0-cot/OXE/droid/1.0.1 (14970:logging_logger.py:49)
2025-09-23 15:38:26.864051: W tensorflow/core/framework/dataset.cc:736] Changing the value of option field deterministic from 0 to 1
15:38:26.864 [W] Changing the value of option deterministic from False to True.                   (14970:options.py:170)
15:38:39.696 [I] [After building lang_table] Memory usage: 331988.67 MB                           (14970:cot_rlds_dataset.py:40)
15:38:39.901 [I] [After building ep_table] Memory usage: 331999.04 MB                             (14970:cot_rlds_dataset.py:40)
15:38:40.625 [I] [After building cam_table] Memory usage: 332048.70 MB                            (14970:cot_rlds_dataset.py:40)
15:38:45.165 [I] [After building instr_table] Memory usage: 332045.21 MB                          (14970:cot_rlds_dataset.py:40)
15:38:45.186 [I] Filter hash table initialized                                                    (14970:cot_rlds_dataset.py:533)
15:38:45.666 [I] Interleaving datasets...                                                         (14970:cot_rlds_dataset.py:1586)
15:38:45.698 [I] Applying frame transforms on dataset...                                          (14970:cot_rlds_dataset.py:1600)
15:38:45.809 [I] After create val loader: RAM: 324.23GB, TPU mem: 50.80GB                         (14970:train.py:483)
15:38:45.823 [I] Before start step: RAM: 324.23GB, TPU mem: 50.80GB                               (14970:train.py:514)

######################################################################################
# Loading the following 4 datasets (incl. sampling weight):                         #
# kuka: ================================================================15342.100000 #
# taco_play: =============================================================892.980000 #
# furniture_bench_dataset_converted_externally_to_rlds: ================11773.368000 #
# droid: =============================================================6809400.000000 #
######################################################################################


######################################################################################
# Loading the following 4 datasets (incl. sampling weight):                         #
# kuka: ================================================================15342.100000 #
# taco_play: =============================================================892.980000 #
# furniture_bench_dataset_converted_externally_to_rlds: ================11773.368000 #
# droid: =============================================================6809400.000000 #
######################################################################################

15:38:45.824 [I] Progress on: -/30000 rate:- remaining:? elapsed:00:00 postfix:-                  (14970:tqdm_logging.py:145)
15:38:45.828 [I] Before first ptrain_step: RAM: 324.23GB, TPU mem: 50.80GB                        (14970:train.py:527)




with berkeley_autolab_ur5 (has depth) 389.09GB

without berkeley_autolab_ur5 (has depth) 381.07GB


only 186.91GB


380.28GB
+ with ram budege 379.77GB
+ limit number threads 187.43G

174.75GB,

+ increase buffer size to 250000  242.89GB



312.88GB